{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hand_Detection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1huE0Xqzw1SphppPvCsTD1YI8yb2kaV2y",
      "authorship_tag": "ABX9TyNFPTRoVpbp/7V2grO4tW+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JhonHader/HAND-KEYPOINTS-DETECTION/blob/main/Hand_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aaxu9dAx_8A4"
      },
      "source": [
        "# <strong>Hand Keypoints Detection</strong>\n",
        "\n",
        "*   **<font color='red'> Description problem </font>** \n",
        "##### Hand keypoints detection and finger count.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "#####Developed by: \n",
        "<h6 align=center> ${\\text{Jhon Hader Fernández}}$ <h6>\n",
        "<h6 align=center> ${\\text{Diego Fernando Díaz}}$ <h6>\n",
        "\n",
        "#####<h6 align=center>{<i>jhon_fernandez, di-diego</i>}@javeriana.edu.co<h6>\n",
        "#####<h6 align=center>Pontificia Universidad Javeriana<h6>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7wIMQJeAWS5"
      },
      "source": [
        "## ***1. ENVIRONMENT***\n",
        "\n",
        "To use this project is recommended to use GPU execution enviroment, to this is so important install all NVIDIA-CUDA dependences (to run deep neural networks [dnn] in OpenCV).\n",
        "This could take a long, therefore we've created a enviroment, to use you have to get its link (it's located in Drive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl3YEiyIlkYh"
      },
      "source": [
        "install_GPU_dependencies = False\n",
        "\n",
        "if install_GPU_dependencies == True:\n",
        "  %cd /content\n",
        "  !git clone https://github.com/opencv/opencv\n",
        "  !git clone https://github.com/opencv/opencv_contrib\n",
        "  !mkdir /content/build\n",
        "  %cd /content/build\n",
        "\n",
        "  !cmake -DOPENCV_EXTRA_MODULES_PATH=/content/opencv_contrib/modules  -DBUILD_SHARED_LIBS=OFF  -DBUILD_TESTS=OFF  -DBUILD_PERF_TESTS=OFF  -DBUILD_EXAMPLES=OFF  -DWITH_OPENEXR=OFF  -DWITH_CUDA=ON  -DWITH_CUBLAS=ON  -DWITH_CUDNN=ON  -DOPENCV_DNN_CUDA=ON  /content/opencv\n",
        "\n",
        "  !make -j8 install\n",
        "\n",
        "  !mkdir  \"/content/drive/My Drive/cv2_cuda\"\n",
        "  !cp  /content/build/lib/python3/cv2.cpython-36m-x86_64-linux-gnu.so \"/content/drive/My Drive/cv2_cuda\"\n",
        "\n",
        "else:\n",
        "  print('[INFO...] CUDA-GPU and OpenCV dependences requirements already installed!')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFheLKBYBQfF"
      },
      "source": [
        "### ***1.1. GET ENVIRONMENT***\n",
        "\n",
        "* **<font color='green'><i> 1.1.1. </i></font>** <br>\n",
        "Get enviroment from Google Drive, please chechk your execution environment, it has to be configurated in **<font color='red'><i> GPU </i></font>**\n",
        "\n",
        "* **<font color='green'><i> 1.1.2. </i></font>** <br>\n",
        "Import libraries \n",
        "<br>\n",
        "\n",
        "\n",
        "**<font color='red'><i> REQUIREMENT </i></font>** <br>\n",
        "OpenCV version has to be greater than **<font color='blue'><i> 4.2.x </i></font>**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAGKgU2sPE_H"
      },
      "source": [
        "GPU = True\n",
        "if GPU == True:\n",
        "  print('[INFO...] Loading GPU dependences.')\n",
        "  !cp \"/content/drive/My Drive/Hand_Detection/cv2_cuda/cv2.cpython-36m-x86_64-linux-gnu.so\" .\n",
        "  print('\\n[INFO...] GPU dependendes was loaded, OpenCV version has to be greater 4.2.x')\n",
        "\n",
        "import cv2\n",
        "from __future__ import division\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import pairwise\n",
        "from skimage import exposure\n",
        "import imutils\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import clear_output\n",
        "print('[INFO...] OpenCV version:', cv2.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZiT3JjRUeuI"
      },
      "source": [
        "## ***2. DATA INPUT (GETTING)***\n",
        "\n",
        "Get video input, get video frames and workspace window."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSN2A_qSFVwf"
      },
      "source": [
        "### ***2.1. GET VIDEO STREAM***\n",
        "\n",
        "You're running code in a cloud server, that isn't your hardware, then, to access to your camera is neccesary use API. This snippet access to your camera and record video stream."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymNYwDa2CJlj"
      },
      "source": [
        "from IPython.display import display, Javascript, HTML\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        " \n",
        "def record_video(filename='video.mp4'):\n",
        "  js = Javascript(\"\"\"\n",
        "    async function recordVideo() {\n",
        "      // mashes together the advanced_outputs.ipynb function provided by Colab, \n",
        "      // a bunch of stuff from Stack overflow, and some sample code from:\n",
        "      // https://developer.mozilla.org/en-US/docs/Web/API/MediaStream_Recording_API\n",
        " \n",
        "      // Optional frames per second argument.\n",
        "      const options = { mimeType: \"video/webm; codecs=vp9\" };\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      const stopCapture = document.createElement(\"button\");\n",
        "      capture.textContent = \"Start Recording\";\n",
        "      capture.style.background = \"green\";\n",
        "      capture.style.color = \"white\";\n",
        " \n",
        "      stopCapture.textContent = \"Stop Recording\";\n",
        "      stopCapture.style.background = \"red\";\n",
        "      stopCapture.style.color = \"white\";\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.id = 'my_canvas';\n",
        "      canvas.height = 480;\n",
        "      canvas.width = 640;\n",
        "      \n",
        "      var ctx = canvas.getContext('2d');\n",
        "      ctx.strokeStyle = 'rgb(255, 255, 0)';  \n",
        "      ctx.lineWidth = 5;\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.id = 'my_video';\n",
        "      video.style.display = 'block';\n",
        "\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      // create a media recorder instance, which is an object\n",
        "      // that will let you record what you stream.\n",
        "      let recorder = new MediaRecorder(stream, options);\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(document.createElement('div'));\n",
        "      div.appendChild(canvas);\n",
        "      // Video is a media element.  This line here sets the object which serves\n",
        "      // as the source of the media associated with the HTMLMediaElement\n",
        "      // Here, we'll set it equal to the stream.\n",
        "      video.srcObject = stream;\n",
        "      // We're inside an async function, so this await will fire off the playing\n",
        "      // of a video. It returns a Promise which is resolved when playback has \n",
        "      // been successfully started. Since this is async, the function will be \n",
        "      // paused until this has started playing. \n",
        "      \n",
        "      // Add event listener to play video\n",
        "      video.addEventListener('play', function(){\n",
        "        draw(this, ctx, canvas.width, canvas.height);\n",
        "      }, false);\n",
        "\n",
        "      // Draw rectangle and flip video\n",
        "      function draw(v, c, w, h) {\n",
        "          c.save();\n",
        "          c.translate(w/2, h/2);\n",
        "          c.scale(-1, 1);\n",
        "          c.drawImage(v, -w/2, -h/2, w, h);\n",
        "          c.restore();\n",
        "          c.strokeRect(300, 50, 280, 300);\n",
        "          setTimeout(draw, 20, v, c, w, h);\n",
        "      }\n",
        "\n",
        "      video.play();\n",
        " \n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "      // and now, just wait for the capture button to get clicked in order to\n",
        "      // start recording\n",
        "      await new Promise((resolve) => {\n",
        "        capture.onclick = resolve;\n",
        "      });\n",
        "      recorder.start();\n",
        "      capture.replaceWith(stopCapture);\n",
        "      // use a promise to tell it to stop recording\n",
        "      await new Promise((resolve) => stopCapture.onclick = resolve);\n",
        "      recorder.stop();\n",
        " \n",
        "      let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n",
        "      let arrBuff = await recData.data.arrayBuffer();\n",
        "      \n",
        "      // stop the stream and remove the video element\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        " \n",
        "      let binaryString = \"\";\n",
        "      let bytes = new Uint8Array(arrBuff);\n",
        "      bytes.forEach((byte) => {\n",
        "        binaryString += String.fromCharCode(byte);\n",
        "      })\n",
        "      return btoa(binaryString);\n",
        "    }\n",
        "    \"\"\")\n",
        "  try:\n",
        "    display(js)\n",
        "    data = eval_js('recordVideo({})')\n",
        "    binary = b64decode(data)\n",
        "    with open(filename, \"wb\") as video_file:\n",
        "      video_file.write(binary)\n",
        "    print(\n",
        "        f\"Finished recording video. Saved binary under filename in current working directory: {filename}\"\n",
        "    )\n",
        "  except Exception as err:\n",
        "      # In case any exceptions arise\n",
        "      print(str(err))\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8E1A0fCGqH-"
      },
      "source": [
        "### ***2.2. GET VIDEO FRAMES***\n",
        "\n",
        "Get all frames in the video, besides shows video information such as: FPS and size. Get FPS is important because we'll need it to renderize a new video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLkl0CPXUkyv"
      },
      "source": [
        "def get_frames_of_video(input_recording):\n",
        "  cap = cv2.VideoCapture(input_recording)\n",
        "\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  fps = 6.25 if fps > 15 else fps    # Sometimes it doesn't get correct FPS, therefore we defined top FPS (15)\n",
        "  size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "\n",
        "  print('[INFO...] Video properties:', 'FPS:', fps, '- (width, height):', (size[0], size[1]))\n",
        "\n",
        "  video = []\n",
        "\n",
        "  print('[INFO...] Getting frames of video.')\n",
        "  \n",
        "  while(cap.isOpened()):\n",
        "      ret, frame = cap.read()\n",
        "      if ret == False:\n",
        "        break\n",
        "      frame = cv2.flip(frame, 1)    \n",
        "      video.append(frame)\n",
        "  \n",
        "  print('[REPORT...] Frames of video was obteined succesfully. \\n')\n",
        "  return video, size, fps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13FzjNnSU6c1"
      },
      "source": [
        "### ***2.3. GET WORKSPACE***\n",
        "\n",
        "Get all video frames in workspace (yellow rectangle) showed when is getting video stream."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRtfQMCv46Zs"
      },
      "source": [
        "def get_workspace(video_frames, window=True):\n",
        "  window_frames = []\n",
        "\n",
        "  if window:\n",
        "    for img in video_frames:\n",
        "      window_frames.append(img[50:350, 300:580].copy())\n",
        "  else:\n",
        "    window_frames = video_frames\n",
        "\n",
        "  return window_frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgJ3cmK7Wr4Q"
      },
      "source": [
        "## ***3. DATA OUTPUT***\n",
        "\n",
        "Data output, renderize video and create it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlcJ9bQXXCrS"
      },
      "source": [
        "### ***3.1. EXPORT VIDEO***\n",
        "\n",
        "Export video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WbB-VxuLomu"
      },
      "source": [
        "import moviepy.video.io.ImageSequenceClip\n",
        "\n",
        "def create_video(frames, fps, filename):\n",
        "  clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(frames, fps=fps)\n",
        "  clip.write_videofile(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxZFEaDbXSWd"
      },
      "source": [
        "### ***3.2. RENDERIZE VIDEO***\n",
        "\n",
        "Renderize video, this is draw join skeleton draw with finger count to main complete frame, let's remember that we are working on workspace window (that's a segment of the complete frame), so, we have to join it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwp8PCMgK-BI"
      },
      "source": [
        "def join_skeleton(video_frames, skeleton_frames, finger_amount, window):\n",
        "\n",
        "  out_frames = []\n",
        "  vid_frame = []\n",
        "\n",
        "  for i, keypoints_frame in enumerate(skeleton_frames):\n",
        "    if window:\n",
        "      vid_frame = cv2.cvtColor(video_frames[i].copy(), cv2.COLOR_BGR2RGB)\n",
        "      vid_frame[50:350, 300:580] = keypoints_frame\n",
        "      if i >= (len(video_frames)-len(finger_amount)):\n",
        "        cv2.putText(vid_frame, finger_amount[i - (len(video_frames) - len(finger_amount))], (5, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), thickness=4)\n",
        "      out_frames.append(vid_frame)\n",
        "    else:\n",
        "      out_frames = skeleton_frames\n",
        "\n",
        "  return out_frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QAM-b8BWLPp"
      },
      "source": [
        "## ***4. SOLUTIONS***\n",
        "\n",
        "We make a proposal, to make two different subsystems to each proccess (finger count and keypoints detection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c4x7hzcWRiX"
      },
      "source": [
        "### ***4.1. FINGER COUNT***\n",
        "\n",
        "Get finger amount. <br>\n",
        "[<font color='blue'><i>**See documentation**<i></font>](https://github.com/JhonHader/HAND-KEYPOINTS-DETECTION)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWBApA7K3qT7"
      },
      "source": [
        "def count_fingers(hand_frames):\n",
        "\n",
        "  bg = None\n",
        "  finger_amount = []\n",
        "\n",
        "  bg = cv2.cvtColor(hand_frames[0], cv2.COLOR_BGR2GRAY)\n",
        "  \n",
        "  for frame in hand_frames[1:]:\n",
        "\n",
        "    # Determinar la región de interes\n",
        "    ROI = frame\n",
        "    grayROI = cv2.cvtColor(ROI, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Región de interés del fondo de la imagen\n",
        "    bgROI = bg\n",
        "\n",
        "    # Determinar la imagen binaria (background vs foreground)\n",
        "    dif = cv2.absdiff(grayROI, bgROI)\n",
        "    _, th = cv2.threshold(dif, 30, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Opening y closing\n",
        "    th = cv2.morphologyEx(th, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8))\n",
        "    th = cv2.morphologyEx(th, cv2.MORPH_CLOSE, np.ones((7, 7), np.uint8))\n",
        "\n",
        "    # Encontrando los contornos de la imagen binaria\n",
        "    cnts, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)[:1]\n",
        "\n",
        "    for cnt in cnts:\n",
        "\n",
        "        # Encontrar el centro del contorno\n",
        "        M = cv2.moments(cnt)\n",
        "        if M[\"m00\"] == 0: M[\"m00\"] = 1\n",
        "        x = int(M[\"m10\"] / M[\"m00\"])\n",
        "        y = int(M[\"m01\"] / M[\"m00\"])\n",
        "\n",
        "        # Encontrar boundary box del contorno\n",
        "        (bnd_x, bnd_y, bnd_w, bnd_h) = cv2.boundingRect(cnt)\n",
        "\n",
        "        # Contorno encontrado a través de cv2.convexHull\n",
        "        hull = cv2.convexHull(cnt)\n",
        "\n",
        "        # Distancias entre maximos puntos y el centro\n",
        "        top = tuple(hull[hull[:, :, 1].argmin()][0])\n",
        "        bottom = tuple(hull[hull[:, :, 1].argmax()][0])\n",
        "        left = tuple(hull[hull[:, :, 0].argmin()][0])\n",
        "        right = tuple(hull[hull[:, :, 0].argmax()][0])\n",
        "        dist = pairwise.euclidean_distances([left, right, top], [[x, y]])\n",
        "        radi = int(0.7 * dist.max())\n",
        "\n",
        "        # Circular ROI\n",
        "        circular_roi = np.zeros(ROI.shape[:-1], dtype=np.uint8)\n",
        "        cv2.circle(circular_roi, (x, y), radi, 255, 8)\n",
        "        fingers = cv2.bitwise_and(th, th, mask=circular_roi)\n",
        "\n",
        "        # Opening\n",
        "        fingers = cv2.morphologyEx(fingers, cv2.MORPH_OPEN, np.ones((7, 7), np.uint8))\n",
        "\n",
        "        # Contour area\n",
        "        fingers_con, _ = cv2.findContours(fingers, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "        finger_count = 0\n",
        "\n",
        "        for counter in fingers_con:\n",
        "            if cv2.contourArea(counter) < 300:\n",
        "                finger_count += 1\n",
        "        \n",
        "        if finger_count > 5:\n",
        "          finger_count = ' '\n",
        "        else:\n",
        "          finger_count = str(finger_count)\n",
        "\n",
        "        finger_amount.append(finger_count)\n",
        "  return finger_amount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H9BQ4q1HYsG"
      },
      "source": [
        "### ***4.2. HAND KEYPOINTS DETECTION***\n",
        "\n",
        "Get all keypoints detection for every frame in the video. <br>\n",
        "[<font color='blue'><i>**See documentation**<i></font>](https://github.com/JhonHader/HAND-KEYPOINTS-DETECTION)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhqdsTqTbhsT"
      },
      "source": [
        "def get_skeleton(hand_frames):\n",
        "  root_path = '/content/drive/My Drive/Hand_Detection'\n",
        "\n",
        "  protoFile = os.path.join(root_path, \"MODELS/pose_deploy.prototxt.txt\")\n",
        "  weightsFile = os.path.join(root_path, \"MODELS/pose_iter_102000.caffemodel\")\n",
        "  nPoints = 22\n",
        "  POSE_PAIRS = [[0, 1], [1, 2], [2, 3], [3, 4], [0, 5], [5, 6], [6, 7], [7, 8], [0, 9],\n",
        "                [9, 10], [10, 11], [11, 12], [0, 13], [13, 14], [14, 15], [15, 16],\n",
        "                [0, 17], [17, 18], [18, 19], [19, 20]]\n",
        "\n",
        "  frameWidth = hand_frames[0].shape[1]\n",
        "  frameHeight = hand_frames[0].shape[0]\n",
        "  aspect_ratio = frameWidth / frameHeight\n",
        "\n",
        "  threshold = 0.1\n",
        "\n",
        "  inHeight = 368\n",
        "  inWidth = int(((aspect_ratio * inHeight) * 8) // 8)\n",
        "\n",
        "  skeleton = []\n",
        "\n",
        "  net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
        "  if GPU == True:\n",
        "    print('[INFO...] GPU was configurated!')\n",
        "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
        "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
        "\n",
        "  t_total = time.time()\n",
        "\n",
        "  print('[INFO...] Processing video, getting hand keypoints!')\n",
        "  for frame in hand_frames: \n",
        "\n",
        "    points = []\n",
        "\n",
        "    img_original = frame.copy()\n",
        "    frameCopy = np.copy(frame)\n",
        "\n",
        "    inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight),\n",
        "                                    (0, 0, 0), swapRB=False, crop=False)\n",
        "\n",
        "    net.setInput(inpBlob)\n",
        "    output = net.forward()\n",
        "    for i in range(nPoints):\n",
        "\n",
        "        probMap = output[0, i, :, :]\n",
        "        probMap = cv2.resize(probMap, (frameWidth, frameHeight))\n",
        "\n",
        "        minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
        "\n",
        "        if prob > threshold:\n",
        "          points.append((int(point[0]), int(point[1])))\n",
        "        else:\n",
        "          points.append(None)\n",
        "\n",
        "    for pair in POSE_PAIRS:\n",
        "        partA, partB = pair[0], pair[1]\n",
        "\n",
        "        if points[partA] and points[partB]:\n",
        "            cv2.line(frame, points[partA], points[partB], (0, 0, 255), 2)\n",
        "            cv2.circle(frame, points[partA], 4, (0, 255, 0), thickness=-1, lineType=cv2.FILLED)\n",
        "            cv2.circle(frame, points[partB], 4, (0, 255, 0), thickness=-1, lineType=cv2.FILLED)\n",
        "\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    skeleton.append(frame)\n",
        "  \n",
        "  print(\"\\n[REPORT...] Hand keypoints was gotten successfully!\")\n",
        "  print(\"[REPORT...] time taken processing video [s] : {:.3f}\".format(time.time() - t_total), '\\n')\n",
        "  return skeleton"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUZZlQknHk6n"
      },
      "source": [
        "## ***5. RESULTS***\n",
        "\n",
        "Get video input, proccess it and get video output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0YNOWH_SG6H"
      },
      "source": [
        "input_recording = 'in.mp4'     \n",
        "record_video(filename=input_recording)\n",
        "\n",
        "window = True\n",
        "\n",
        "video_frames, size, fps = get_frames_of_video(input_recording)\n",
        "window_frames = get_workspace(video_frames, window=window)\n",
        "finger_amount = count_fingers(hand_frames=window_frames)\n",
        "skeleton_frames = get_skeleton(window_frames)\n",
        "out_frames = join_skeleton(video_frames, skeleton_frames, finger_amount, window=window)\n",
        "create_video(out_frames, fps, 'out.mp4')\n",
        "\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "video_width = 640\n",
        "video_file = open('out.mp4', \"r+b\").read()\n",
        "video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}