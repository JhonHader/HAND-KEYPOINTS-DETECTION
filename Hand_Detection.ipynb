{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hand_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1huE0Xqzw1SphppPvCsTD1YI8yb2kaV2y",
      "authorship_tag": "ABX9TyNstvMpN5hqaRs38AVYg8cJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JhonHader/HAND-KEYPOINTS-DETECTION/blob/main/Hand_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aaxu9dAx_8A4"
      },
      "source": [
        "# <strong>Hand Keypoints Detection</strong>\n",
        "\n",
        "*   **<font color='red'> Description problem </font>** \n",
        "##### Hand-detection and its keypoints.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "#####Developed by: \n",
        "<h6 align=center> ${\\text{Jhon Hader Fernández}}$ <h6>\n",
        "<h6 align=center> ${\\text{Diego Fernando Díaz}}$ <h6>\n",
        "\n",
        "#####<h6 align=center>{<i>jhon_fernandez, di-diego</i>}@javeriana.edu.co<h6>\n",
        "#####<h6 align=center>Pontificia Universidad Javeriana<h6>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7wIMQJeAWS5"
      },
      "source": [
        "## ***1. ENVIRONMENT***\n",
        "\n",
        "To use this project is recommended use GPU execution enviroment, to this is so important install all NVIDIA-CUDA dependences (to run deep neural networks in OpenCV).\n",
        "This could take a long, therefore we've created a enviroment, to use you have to get its link (it's located in Drive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl3YEiyIlkYh"
      },
      "source": [
        "install_GPU_dependencies = False\n",
        "\n",
        "if install_GPU_dependencies == True:\n",
        "  %cd /content\n",
        "  !git clone https://github.com/opencv/opencv\n",
        "  !git clone https://github.com/opencv/opencv_contrib\n",
        "  !mkdir /content/build\n",
        "  %cd /content/build\n",
        "\n",
        "  !cmake -DOPENCV_EXTRA_MODULES_PATH=/content/opencv_contrib/modules  -DBUILD_SHARED_LIBS=OFF  -DBUILD_TESTS=OFF  -DBUILD_PERF_TESTS=OFF  -DBUILD_EXAMPLES=OFF  -DWITH_OPENEXR=OFF  -DWITH_CUDA=ON  -DWITH_CUBLAS=ON  -DWITH_CUDNN=ON  -DOPENCV_DNN_CUDA=ON  /content/opencv\n",
        "\n",
        "  !make -j8 install\n",
        "\n",
        "  !mkdir  \"/content/drive/My Drive/cv2_cuda\"\n",
        "  !cp  /content/build/lib/python3/cv2.cpython-36m-x86_64-linux-gnu.so \"/content/drive/My Drive/cv2_cuda\"\n",
        "\n",
        "else:\n",
        "  print('[INFO...] CUDA-GPU and OpenCV dependences requirements already installed!')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFheLKBYBQfF"
      },
      "source": [
        "### ***1.1. GET ENVIRONMENT***\n",
        "\n",
        "* **<font color='green'><i> 1.1.1. </i></font>** <br>\n",
        "Get enviroment from Google Drive, please chechk your execution environment, it has to be configurated in **<font color='red'><i> GPU </i></font>**\n",
        "\n",
        "* **<font color='green'><i> 1.1.2. </i></font>** <br>\n",
        "Import libraries \n",
        "<br>\n",
        "\n",
        "\n",
        "**<font color='red'><i> REQUIREMENT </i></font>** <br>\n",
        "OpenCV version has to be greater than **<font color='blue'><i> 4.2.x </i></font>**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAGKgU2sPE_H"
      },
      "source": [
        "GPU = True\n",
        "if GPU == True:\n",
        "  print('[INFO...] Loading GPU dependences.')\n",
        "  !cp \"/content/drive/My Drive/cv2_cuda/cv2.cpython-36m-x86_64-linux-gnu.so\" .\n",
        "  print('\\n[INFO...] GPU dependendes was loaded, OpenCV version has to be greater 4.2.x')\n",
        "\n",
        "import cv2\n",
        "from __future__ import division\n",
        "import time\n",
        "import numpy as np\n",
        "from skimage import exposure\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import clear_output\n",
        "print('[INFO...] OpenCV version:', cv2.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSN2A_qSFVwf"
      },
      "source": [
        "## ***2. GET VIDEO STREAM***\n",
        "\n",
        "You're running code in a cloud server, that isn't your hardware, then, to access to your camera is neccesary use API. This snippet access to your camera and record video stream."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymNYwDa2CJlj"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def record_video(filename='video.mp4'):\n",
        "  js = Javascript(\"\"\"\n",
        "    async function recordVideo() {\n",
        "      // mashes together the advanced_outputs.ipynb function provided by Colab, \n",
        "      // a bunch of stuff from Stack overflow, and some sample code from:\n",
        "      // https://developer.mozilla.org/en-US/docs/Web/API/MediaStream_Recording_API\n",
        "\n",
        "      // Optional frames per second argument.\n",
        "      const options = { mimeType: \"video/webm; codecs=vp9\" };\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      const stopCapture = document.createElement(\"button\");\n",
        "      capture.textContent = \"Start Recording\";\n",
        "      capture.style.background = \"green\";\n",
        "      capture.style.color = \"white\";\n",
        "\n",
        "      stopCapture.textContent = \"Stop Recording\";\n",
        "      stopCapture.style.background = \"red\";\n",
        "      stopCapture.style.color = \"white\";\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      const recordingVid = document.createElement(\"video\");\n",
        "      video.style.display = 'block';\n",
        "\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      // create a media recorder instance, which is an object\n",
        "      // that will let you record what you stream.\n",
        "      let recorder = new MediaRecorder(stream, options);\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      // Video is a media element.  This line here sets the object which serves\n",
        "      // as the source of the media associated with the HTMLMediaElement\n",
        "      // Here, we'll set it equal to the stream.\n",
        "      video.srcObject = stream;\n",
        "      // We're inside an async function, so this await will fire off the playing\n",
        "      // of a video. It returns a Promise which is resolved when playback has \n",
        "      // been successfully started. Since this is async, the function will be \n",
        "      // paused until this has started playing. \n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "      // and now, just wait for the capture button to get clicked in order to\n",
        "      // start recording\n",
        "      await new Promise((resolve) => {\n",
        "        capture.onclick = resolve;\n",
        "      });\n",
        "      recorder.start();\n",
        "      capture.replaceWith(stopCapture);\n",
        "      // use a promise to tell it to stop recording\n",
        "      await new Promise((resolve) => stopCapture.onclick = resolve);\n",
        "      recorder.stop();\n",
        "\n",
        "      let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n",
        "      let arrBuff = await recData.data.arrayBuffer();\n",
        "      \n",
        "      // stop the stream and remove the video element\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "\n",
        "      let binaryString = \"\";\n",
        "      let bytes = new Uint8Array(arrBuff);\n",
        "      bytes.forEach((byte) => {\n",
        "        binaryString += String.fromCharCode(byte);\n",
        "      })\n",
        "      return btoa(binaryString);\n",
        "    }\n",
        "    \"\"\")\n",
        "  try:\n",
        "    display(js)\n",
        "    data = eval_js('recordVideo({})')\n",
        "    binary = b64decode(data)\n",
        "    with open(filename, \"wb\") as video_file:\n",
        "      video_file.write(binary)\n",
        "    print(\n",
        "        f\"Finished recording video. Saved binary under filename in current working directory: {filename}\"\n",
        "    )\n",
        "  except Exception as err:\n",
        "      # In case any exceptions arise\n",
        "      print(str(err))\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LexilkRF4uW"
      },
      "source": [
        "### ***2.1. RECORD VIDEO STREAM***\n",
        "\n",
        "Record a video stream, this recording is saved in local enviroment, when you close the session that video will be deleted and all data in local environment too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KEQ_-4RUdaQ"
      },
      "source": [
        "input_recording = 'mirror_test.avi'     # filename to save recording, you should save it in '.avi' format\n",
        "record_video(filename=input_recording)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2WHXWPFGbqS"
      },
      "source": [
        "### ***2.2. PLAY VIDEO RECORDED***\n",
        "\n",
        "Play video stream recorded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUuxgTORUgKp"
      },
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "video_width = 800\n",
        "video_file = open(input_recording, \"r+b\").read()\n",
        "video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8E1A0fCGqH-"
      },
      "source": [
        "## ***3. GET FRAMES OF VIDEO***\n",
        "\n",
        "Get all frames in the video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLkl0CPXUkyv"
      },
      "source": [
        "cap = cv2.VideoCapture(input_recording)\n",
        "\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "(width, height) = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "\n",
        "print(fps, (width, height))\n",
        "\n",
        "video = []\n",
        "\n",
        "print('[INFO...] Getting frames of video.')\n",
        "while(cap.isOpened()):\n",
        "    ret, frame = cap.read()\n",
        "    if ret == False:\n",
        "      break\n",
        "    video.append(frame)\n",
        "print('[REPORT...] Frames of video was obteined succesfully.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H9BQ4q1HYsG"
      },
      "source": [
        "## ***4. HAND KEYPOINTS DETECTION***\n",
        "\n",
        "Get all keypoints detection for every frame in the video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhqdsTqTbhsT"
      },
      "source": [
        "root_path = '/content/drive/My Drive/Hand_Detection'\n",
        "\n",
        "protoFile = os.path.join(root_path, \"MODELS/pose_deploy.prototxt.txt\")\n",
        "weightsFile = os.path.join(root_path, \"MODELS/pose_iter_102000.caffemodel\")\n",
        "nPoints = 22\n",
        "POSE_PAIRS = [[0, 1], [1, 2], [2, 3], [3, 4], [0, 5], [5, 6], [6, 7], [7, 8], [0, 9],\n",
        "              [9, 10], [10, 11], [11, 12], [0, 13], [13, 14], [14, 15], [15, 16],\n",
        "              [0, 17], [17, 18], [18, 19], [19, 20]]\n",
        "\n",
        "skeleton = []\n",
        "\n",
        "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
        "if GPU == True:\n",
        "  print('[INFO...] GPU was configurated!')\n",
        "  net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
        "  net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
        "\n",
        "frameWidth = width\n",
        "frameHeight = height\n",
        "aspect_ratio = frameWidth / frameHeight\n",
        "\n",
        "threshold = 0.1\n",
        "\n",
        "inHeight = 368\n",
        "inWidth = int(((aspect_ratio * inHeight) * 8) // 8)\n",
        "\n",
        "t_total = time.time()\n",
        "\n",
        "print('[INFO...] Processing video, getting hand keypoints!')\n",
        "for frame in video: \n",
        "\n",
        "  points = []\n",
        "\n",
        "  img_original = frame.copy()\n",
        "  frameCopy = np.copy(frame)\n",
        "\n",
        "  inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight),\n",
        "                                  (0, 0, 0), swapRB=False, crop=False)\n",
        "\n",
        "  net.setInput(inpBlob)\n",
        "  output = net.forward()\n",
        "  for i in range(nPoints):\n",
        "\n",
        "      probMap = output[0, i, :, :]\n",
        "      probMap = cv2.resize(probMap, (frameWidth, frameHeight))\n",
        "\n",
        "      minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
        "\n",
        "      if prob > threshold:\n",
        "        points.append((int(point[0]), int(point[1])))\n",
        "      else:\n",
        "        points.append(None)\n",
        "\n",
        "  for pair in POSE_PAIRS:\n",
        "      partA, partB = pair[0], pair[1]\n",
        "\n",
        "      if points[partA] and points[partB]:\n",
        "          cv2.line(frame, points[partA], points[partB], (0, 0, 255), 2)\n",
        "          cv2.circle(frame, points[partA], 4, (0, 255, 0), thickness=-1, lineType=cv2.FILLED)\n",
        "          cv2.circle(frame, points[partB], 4, (0, 255, 0), thickness=-1, lineType=cv2.FILLED)\n",
        "\n",
        "  skeleton.append(frame)\n",
        "print(\"\\n[REPORT...] Hand keypoints was gotten successfully!\")\n",
        "print(\"[REPORT...] time taken processing video [s] : {:.3f}\".format(time.time() - t_total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUZZlQknHk6n"
      },
      "source": [
        "## ***5. SHOW KEYPOINTS***\n",
        "\n",
        "Show keypoints for every frame in the video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zee0Zti0LkS6"
      },
      "source": [
        "%pylab inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for frame in skeleton:\n",
        "  #plt.figure(figsize = (height*0.015, width*0.015))\n",
        "  plt.imshow(frame)\n",
        "  plt.show()\n",
        "  time.sleep(1//6.25)\n",
        "  clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}